## Situation
Around 35% GPU utilization for bq-1 on NVIDIA RTX A4000. Why not above 90% ? What are the bottlenecks in the llama.cpp inference pipeline for qwq LLM? How to resolve them to unlock the maximum GPU utilization performance?
## Task
## Action
## Result
## Retrospectives
